# Virtual Private Cloud Control & Data plane,Powered by ASAP^2 and SR-IOV 
[![CI](https://github.com/fatih881/ebpf-fw/actions/workflows/main.yml/badge.svg)](https://github.com/fatih881/ebpf-vpc/actions/workflows/main.yml)
[![Go Report Card](https://goreportcard.com/badge/github.com/fatih881/ebpf-fw)](https://goreportcard.com/report/github.com/fatih881/ebpf-vpc)
[![codecov](https://codecov.io/gh/fatih881/ebpf-fw/graph/badge.svg?token=IVF4HTGMWB)](https://codecov.io/gh/fatih881/ebpf-vpc)    
> **Status:** Under development,No releases   

### The implementation utilizes a Split Data Plane to offload virtual networking tasks from the host CPU to the NIC ASIC:

    Slow Path (Kernel-space): Initial flow packets are handled by XDP or TC hooks. The Control Plane performs tenant policy validation, routing resolution, and SR-IOV/VNI mapping.

    Fast Path (Hardware Offload): Flow rules are pushed to the NIC eSwitch via the TC Flower protocol. Subsequent packets are processed at line-rate by the hardware ASIC.

    I/O Path: Leverages SR-IOV Virtual Functions (VFs) for directly transporting packets to workloads.
Diagram generated by Gemini:
![diagram.png](diagram.png)
> Diagram includes a "ConnectX-5",because ASAP^2 is supported in ConnectX-5 and newer NICs by Mellanox/NVDIA.  
### Roadmap & TODO
- [ ] Self-Hosted test environment with related features supported NIC,ansible & packer usage is a must.
- [ ] Orchestrating assigning & mapping VXLAN & Virtual Functions with workloads.
- [ ] Hardware-offloaded data plane with TC flower to bypass CPU & Host Kernel while transporting packages to workloads.
- [ ] gRPC API's for editing firewall rules per tenant.  
"Single Root IO Virtualization (SR-IOV) is a technology that allows a physical PCIe device to present itself multiple times through the PCIe bus."
see [official docs](https://docs.nvidia.com/doca/archive/2-9-2/single+root+io+virtualization+(sr-iov)/index.html)  for details.  
"NVIDIA Accelerated Switching And Packet Processing (ASAP2) technology allows OVS offloading by handling OVS data-plane in ConnectX-5 onwards NIC hardware (Embedded Switch or eSwitch) while maintaining OVS control-plane unmodified. As a result, we observe significantly higher OVS performance without the associated CPU load."
see [official docs](https://docs.nvidia.com/networking/display/mlnxofedv24103250lts/ovs+offload+using+asap%C2%B2+direct#) for details.